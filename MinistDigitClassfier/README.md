# k-近邻算法识别手写数字数据集

* 目录trainingDigits中包含大约2000个例子，每个数字大约有200个样本；目录testDigits中包含大约900个测试例子。使用目录trainingDigits中的数据训练分类器，使用目录testDigits
  中数据测试分类器的效果。
  
* 在KNN.py代码文件中：首先编写一个函数img2vector()将图像转化为向量，把32*32的二进制图像矩阵转化为1*1024的向量。函数classify0()函数有4个输入参数：用于分类的输入向量
  inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数表示用于选择最近邻居的数目，其中标签向量的元素数据和矩阵dataSet的行数相同。此函数使用**欧式距离公式**
  计算两个向量之间的距离。计算完所用点之间的距离之后就可以对数据按照从小到大的次序排序。然后确定k个距离最小的元素所在的主要分类；最后将classCount字典分解为元组列表
  ，然后使用程序第二行导入运算符模块的itemgetter方法，按照第二个元素的次序对元祖进行排序。此处的排序为逆序，最后返回发生频率最高的元素标签。函数handwritingClassTest()
  首先创建一个m行1024列的训练矩阵，该矩阵的每行存储一个图像。可以从文件名中解析出分类数字。在下一步中，对testDigits目录中的文件执行相似的操作，不同的是并不需要将这个
  目录下的文件载入矩阵中，而是使用classify0()函数测试该目录下的每个文件。
  
* 错误率为1.2%，改变变量k的值，修改函数handwritingClassTest随机选取训练样本，改变训练样本的数目，都会对k-近邻的错误率产生影响。
实际使用这个算法时，算法的执行效率并不高。因为算法需要每个测试向量做2000次距离计算，每个距离计算包括1024个维度的浮点运算。此外，还需要为测试向量准备2M的文存储空间，
是否存在一种算法减小存储空间和计算时间的开销呢？k决策树就是k-近邻算法的改进版，可以节省大量的计算开销。

# k-近邻算法小结
* k-近邻算法是分类数据最简单最有效的算法。k-邻近算法必须保存数据集中的每个数据计算特征值，实际使用时可能非常**耗时**。