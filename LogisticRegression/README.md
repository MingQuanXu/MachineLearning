# 运行环境
* 运行平台： Windows 
* Python版本： Python2.x 
* IDE： **anaconda** + Pycharm

# 基于Logistic回归和Sigmoid函数的分类
* testSet.txt是数据集共有100个样本点，每个点包含两个数值型特征：X1和X2。在此数据集上，通过**梯度上升算法**找到最佳回归系数，也就是拟合出Logistic回归模型的最佳参数（回归系数）。
* LR.py是代码文件。函数loadDataSet() 主要功能打开文件 “testSet.txt” 并逐行读取。每行前两个值分别是X1和X2，第三个数据对应类别标签。此外为了计算方便，该函数还将X0的值设置为1.0。
  梯度上升算法的实际工作在函数gradAscent()里完成，该函数有两个参数。第一个参数是dataMathIn，它是一个二维数组，每列分别代表不同的特征，每行则代表训练样本。现在采用
  的是100个样本的简单数据集，它包含两个特征X1和X2，再加上第0维特征X0，所以dataMathIn里存放的将是100*3的矩阵。第二个参数是类标签，它是一个1*100的行向量。为了便于
  矩阵计算，需要将该行向量转化为列向量，做法是将原向量转置，再将它赋值给labelMat。接下来的代码是得到矩阵的大小，再设置一些梯度上升算法所需要的参数。变量alpha是向目标
  移动的步长，maxCycles是迭代次数。在for循环迭代完成后将返回训练好的回归系数。变量h不是一个数而是一个列向量，列向量的元素个数等于样本的个数，这里是100，对应的运算
  dataMatrix * weights代表的不止一次乘积运算，事实上该运算包含了300次的乘积运算。回归系数确定了不同类别数据之间的分割线，函数plotBestFit()画出了该分割线，从而使得
  优化的过程便于理解。
* 使用时把两个文件放在用一个目录下。运行结果如图所示：![](https://github.com/MingQuanXu/MachineLearning/blob/master/LogisticRegression/LR.png)